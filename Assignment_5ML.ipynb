{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 5**"
      ],
      "metadata": {
        "id": "TV3v8amnbc0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. (Based on Step-by-Step Implementation of Ridge Regression using Gradient\n",
        "Descent Optimization)\n",
        "Generate a dataset with atleast seven highly correlated columns and a target variable.\n",
        "Implement Ridge Regression using Gradient Descent Optimization. Take different values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization parameter (10^-15,10^-10,10^-5 ,10^-3\n",
        ",0,1,10,20). Choose the best parameters for which ridge\n",
        "regression cost function is minimum and R2_score is maximum."
      ],
      "metadata": {
        "id": "ql9qzt1lYCf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyiqSQSCXDl_",
        "outputId": "7c604c6a-9066-4f47-fc4b-76c39de6cb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'lr': 0.1, 'lam': 1e-15, 'r2': 0.9926028747850965, 'cost': np.float64(0.008990246761538433)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-807818155.py:25: RuntimeWarning: overflow encountered in square\n",
            "  cost = np.mean((y - y_pred)**2) + lam*np.sum(w**2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 7)\n",
        "y = 3*X[:,0] + 2*X[:,1] + X[:,2] + np.random.randn(100) * 0.1\n",
        "X = np.hstack([X, X[:, [0]] * 0.9])  # add correlation\n",
        "\n",
        "# Ridge Regression via Gradient Descent\n",
        "def ridge_gradient_descent(X, y, lr, lam, iterations=1000):\n",
        "    n, m = X.shape\n",
        "    X = np.c_[np.ones((n, 1)), X]\n",
        "    w = np.zeros(m+1)\n",
        "    for _ in range(iterations):\n",
        "        y_pred = X.dot(w)\n",
        "        grad = (-2/n)*X.T.dot(y - y_pred) + 2*lam*w\n",
        "        grad[0] -= 2*lam*w[0]  # no regularization for bias\n",
        "        w -= lr * grad\n",
        "\n",
        "        # stop if weights blow up\n",
        "        if np.any(np.isnan(w)) or np.any(np.isinf(w)):\n",
        "            return w, np.inf\n",
        "    cost = np.mean((y - y_pred)**2) + lam*np.sum(w**2)\n",
        "    return w, cost\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "lambdas = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "best = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        w, cost = ridge_gradient_descent(X, y, lr, lam)\n",
        "        if np.isinf(cost):  # skip broken combos\n",
        "            continue\n",
        "        y_pred = np.c_[np.ones((X.shape[0],1)), X].dot(w)\n",
        "        if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n",
        "            continue\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        if not best or r2 > best['r2']:\n",
        "            best = {'lr': lr, 'lam': lam, 'r2': r2, 'cost': cost}\n",
        "\n",
        "print(\"Best Parameters:\", best)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Load the Hitters dataset from the following link\n",
        "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing\n",
        "\n",
        "(a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
        "\n",
        "(b) Separate input and output features and perform scaling\n",
        "\n",
        "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use regularization parameter as 0.5748) regression function on the dataset.\n",
        "\n",
        "(d) Evaluate the performance of each trained model on test set. Which model\n",
        "performs the best and Why?"
      ],
      "metadata": {
        "id": "TbKpfZfFYZUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('Hitters.csv')\n",
        "data = data.dropna()\n",
        "\n",
        "# Encode categorical features\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "# Split\n",
        "X = data.drop(\"Salary\", axis=1)\n",
        "y = data[\"Salary\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Models\n",
        "models = {\n",
        "    \"Linear\": LinearRegression(),\n",
        "    \"Ridge\": Ridge(alpha=0.5748),\n",
        "    \"Lasso\": Lasso(alpha=0.5748)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    print(f\"{name} R2 Score:\", r2_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy4zYHsdXwF6",
        "outputId": "db7faad4-8af8-49bb-fce3-60144baecbab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear R2 Score: 0.16769360190025295\n",
            "Ridge R2 Score: 0.169752366183749\n",
            "Lasso R2 Score: 0.14682108115827563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Cross Validation for Ridge and Lasso Regression\n",
        "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)function of Python. Implement both on Boston House Prediction Dataset (load_boston dataset from sklearn.datasets)."
      ],
      "metadata": {
        "id": "4-A5mrR4YpcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load California housing dataset instead of Boston\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Ridge and Lasso with cross-validation\n",
        "ridge_cv = RidgeCV(alphas=[0.1, 1, 10])\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "\n",
        "lasso_cv = LassoCV(alphas=[0.1, 1, 10])\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best Ridge alpha:\", ridge_cv.alpha_)\n",
        "print(\"Ridge R2:\", r2_score(y_test, ridge_cv.predict(X_test)))\n",
        "\n",
        "print(\"Best Lasso alpha:\", lasso_cv.alpha_)\n",
        "print(\"Lasso R2:\", r2_score(y_test, lasso_cv.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6L-TNkqX8gJ",
        "outputId": "c399bf5c-ee47-41a8-8b8f-270d7ca2585a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Ridge alpha: 10.0\n",
            "Ridge R2: 0.5764371557026509\n",
            "Best Lasso alpha: 0.1\n",
            "Lasso R2: 0.5318167610318159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Multiclass Logistic Regression: Implement Multiclass Logistic Regression (step-by step) on Iris dataset using one vs. rest strategy?"
      ],
      "metadata": {
        "id": "xNjQrj7jY4Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def train_one_vs_rest(X, y, cls, lr=0.01, iters=1000):\n",
        "    y_bin = np.where(y == cls, 1, 0)\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    for _ in range(iters):\n",
        "        z = np.dot(X, w) + b\n",
        "        y_pred = sigmoid(z)\n",
        "        dw = np.dot(X.T, (y_pred - y_bin)) / len(y)\n",
        "        db = np.mean(y_pred - y_bin)\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "    return w, b\n",
        "\n",
        "weights, biases = [], []\n",
        "for cls in np.unique(y_train):\n",
        "    w, b = train_one_vs_rest(X_train, y_train, cls)\n",
        "    weights.append(w)\n",
        "    biases.append(b)\n",
        "\n",
        "def predict(X):\n",
        "    probs = [sigmoid(X.dot(w) + b) for w, b in zip(weights, biases)]\n",
        "    return np.argmax(probs, axis=0)\n",
        "\n",
        "y_pred = predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVl3gGSmX9zl",
        "outputId": "41bfc105-4f33-4945-cd3b-2f5d0e948bd7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9\n"
          ]
        }
      ]
    }
  ]
}