{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This assignment will help you practice web scraping techniques by extracting structured data\n",
        "from a live practice website. You will learn how to navigate HTML structures, extract relevant\n",
        "information, and save it in a structured format for analysis."
      ],
      "metadata": {
        "id": "7oeZ0B1tdO3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUES 1**"
      ],
      "metadata": {
        "id": "wlJS8ciRddpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to scrape all available books from the website\n",
        "(https://books.toscrape.com/) Books to Scrape - a live site built for practicing scraping (safe,legal, no anti-bot). For each book, extract the following details:\n",
        "\n",
        "1. Title\n",
        "\n",
        "2. Price\n",
        "\n",
        "3. Availability (In stock / Out of stock)\n",
        "\n",
        "4. Star Rating (One, Two, Three, Four, Five)\n",
        "\n",
        "Store the scraped results into a Pandas DataFrame and export them to a CSV file named books.csv.\n",
        "\n",
        "(Note: Use the requests library to fetch the HTML page. Use BeautifulSoup to parse and extract book details and handle pagination so that books from all pages are scraped)"
      ],
      "metadata": {
        "id": "eTn2Whhqdh-6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NsnyOLmdJkn",
        "outputId": "ccfabe69-21c6-49fd-a51c-cb576b60d5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 scraped successfully...\n",
            "Page 2 scraped successfully...\n",
            "Page 3 scraped successfully...\n",
            "Page 4 scraped successfully...\n",
            "Page 5 scraped successfully...\n",
            "Page 6 scraped successfully...\n",
            "Page 7 scraped successfully...\n",
            "Page 8 scraped successfully...\n",
            "Page 9 scraped successfully...\n",
            "Page 10 scraped successfully...\n",
            "Page 11 scraped successfully...\n",
            "Page 12 scraped successfully...\n",
            "Page 13 scraped successfully...\n",
            "Page 14 scraped successfully...\n",
            "Page 15 scraped successfully...\n",
            "Page 16 scraped successfully...\n",
            "Page 17 scraped successfully...\n",
            "Page 18 scraped successfully...\n",
            "Page 19 scraped successfully...\n",
            "Page 20 scraped successfully...\n",
            "Page 21 scraped successfully...\n",
            "Page 22 scraped successfully...\n",
            "Page 23 scraped successfully...\n",
            "Page 24 scraped successfully...\n",
            "Page 25 scraped successfully...\n",
            "Page 26 scraped successfully...\n",
            "Page 27 scraped successfully...\n",
            "Page 28 scraped successfully...\n",
            "Page 29 scraped successfully...\n",
            "Page 30 scraped successfully...\n",
            "Page 31 scraped successfully...\n",
            "Page 32 scraped successfully...\n",
            "Page 33 scraped successfully...\n",
            "Page 34 scraped successfully...\n",
            "Page 35 scraped successfully...\n",
            "Page 36 scraped successfully...\n",
            "Page 37 scraped successfully...\n",
            "Page 38 scraped successfully...\n",
            "Page 39 scraped successfully...\n",
            "Page 40 scraped successfully...\n",
            "Page 41 scraped successfully...\n",
            "Page 42 scraped successfully...\n",
            "Page 43 scraped successfully...\n",
            "Page 44 scraped successfully...\n",
            "Page 45 scraped successfully...\n",
            "Page 46 scraped successfully...\n",
            "Page 47 scraped successfully...\n",
            "Page 48 scraped successfully...\n",
            "Page 49 scraped successfully...\n",
            "Page 50 scraped successfully...\n",
            "Scraping completed! Data saved to books.csv\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
        "\n",
        "# Empty list to store book data\n",
        "books_data = []\n",
        "\n",
        "# Loop through all pages\n",
        "page = 1\n",
        "while True:\n",
        "    # Fetch the page\n",
        "    url = base_url.format(page)\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if page exists\n",
        "    if response.status_code != 200:\n",
        "        break                     # Exit loop if no more pages\n",
        "\n",
        "    # Parse HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all books on the page\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    # Extract details for each book\n",
        "    for book in books:\n",
        "        title = book.h3.a['title']  # Book title\n",
        "        price = book.find('p', class_='price_color').text  # Price\n",
        "        availability = book.find('p', class_='instock availability').text.strip()  # Availability\n",
        "        star_rating = book.p['class'][1]  # Star rating (class contains 'star-rating One', 'Two', etc.)\n",
        "\n",
        "        # Add book data to the list\n",
        "        books_data.append({\n",
        "            'Title': title,\n",
        "            'Price': price,\n",
        "            'Availability': availability,\n",
        "            'Star Rating': star_rating\n",
        "        })\n",
        "\n",
        "    print(f\"Page {page} scraped successfully...\")\n",
        "    page += 1  # Move to the next page\n",
        "\n",
        "# Convert list to Pandas DataFrame\n",
        "df = pd.DataFrame(books_data)\n",
        "\n",
        "# Export DataFrame to CSV\n",
        "df.to_csv('books.csv', index=False)\n",
        "\n",
        "print(\"Scraping completed! Data saved to books.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUES 2**"
      ],
      "metadata": {
        "id": "iALTh6mkpuFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to scrape the IMDB Top 250 Movies list\n",
        "(https://www.imdb.com/chart/top/) .\n",
        "\n",
        "For each movie, extract the following details:\n",
        "\n",
        "1. Rank (1-250)\n",
        "\n",
        "2. Movie Title\n",
        "\n",
        "3. Year of Release\n",
        "\n",
        "4. IMDB Rating\n",
        "\n",
        "Store the results in a Pandas DataFrame and export it to a CSV file named imdb_top250.csv.\n",
        "\n",
        "(Note: Use Selenium/Playwright to scrape the required details from this website)"
      ],
      "metadata": {
        "id": "v8GQeqYzsG6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "\n",
        "# Configure Selenium options\n",
        "chrome_opts = Options()\n",
        "chrome_opts.add_argument(\"--headless\")\n",
        "chrome_opts.add_argument(\"--no-sandbox\")\n",
        "chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Spoof user-agent to avoid 403 Forbidden\n",
        "chrome_opts.add_argument(\n",
        "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "    \"Chrome/115.0.0.0 Safari/537.36\"\n",
        ")\n",
        "\n",
        "# Launch browser\n",
        "browser = webdriver.Chrome(options=chrome_opts)\n",
        "\n",
        "# Open IMDb Top 250\n",
        "browser.get(\"https://www.imdb.com/chart/top/\")\n",
        "time.sleep(5)  # wait for page to load\n",
        "\n",
        "film_list = []\n",
        "movie_cards = browser.find_elements(By.CSS_SELECTOR, \".ipc-metadata-list-summary-item\")\n",
        "\n",
        "# Extract movie details\n",
        "for rank, card in enumerate(movie_cards, start=1):\n",
        "    try:\n",
        "        name = card.find_element(By.CSS_SELECTOR, \"h3\").text\n",
        "        release_year = card.find_element(By.CSS_SELECTOR, \".cli-title-metadata-item\").text\n",
        "        score = card.find_element(By.CSS_SELECTOR, \".ipc-rating-star--imdb\").text.split()[0]\n",
        "        film_list.append([rank, name, release_year, score])\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping a card due to error: {e}\")\n",
        "\n",
        "browser.quit()\n",
        "\n",
        "# Save as DataFrame\n",
        "imdb_table = pd.DataFrame(film_list, columns=[\"Position\", \"Movie\", \"Release Year\", \"Rating\"])\n",
        "imdb_table.to_csv(\"imdb_top250.csv\", index=False)\n",
        "print(imdb_table.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chI2d6cLh3JO",
        "outputId": "cd95202f-7e9b-4514-eb6d-6acd7fef6be2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (4.35.0)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "   Position                        Movie Release Year Rating\n",
            "0         1  1. The Shawshank Redemption         1994    9.3\n",
            "1         2             2. The Godfather         1972    9.2\n",
            "2         3           3. The Dark Knight         2008    9.1\n",
            "3         4     4. The Godfather Part II         1974    9.0\n",
            "4         5              5. 12 Angry Men         1957    9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUES 3**"
      ],
      "metadata": {
        "id": "T4kbHuen5YqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to scrape the weather information for top world cities from the given website (https://www.timeanddate.com/weather/). For each city, extract the following details:\n",
        "\n",
        "1. City Name\n",
        "\n",
        "2. Temperature\n",
        "\n",
        "3. Weather Condition (e.g., Clear, Cloudy, Rainy, etc.)\n",
        "\n",
        "Store the results in a Pandas DataFrame and export it to a CSV file named weather.csv."
      ],
      "metadata": {
        "id": "H6NelBj25NSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "\n",
        "# Configure Selenium options\n",
        "chrome_opts = Options()\n",
        "chrome_opts.add_argument(\"--headless\")\n",
        "chrome_opts.add_argument(\"--no-sandbox\")\n",
        "chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Spoof user-agent to avoid 403 Forbidden\n",
        "chrome_opts.add_argument(\n",
        "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "    \"Chrome/115.0.0.0 Safari/537.36\"\n",
        ")\n",
        "\n",
        "# Launch browser\n",
        "browser = webdriver.Chrome(options=chrome_opts)\n",
        "\n",
        "# Open IMDb Top 250\n",
        "browser.get(\"https://www.timeanddate.com/weather/?sort=1&low=4\")\n",
        "time.sleep(5)  # wait for page to load\n",
        "\n",
        "weather_list = []\n",
        "weather_rows = browser.find_elements(By.CSS_SELECTOR, \"table.zebra.fw.tb-theme tbody tr\")\n",
        "\n",
        "# Extract movie details\n",
        "for row in weather_rows:\n",
        "    try:\n",
        "        city_name = row.find_element(By.CSS_SELECTOR, \"td:nth-child(1)\").text\n",
        "\n",
        "        cond_td = row.find_element(By.CSS_SELECTOR, \"td:nth-child(3)\")\n",
        "        condition_img = cond_td.find_element(By.TAG_NAME, \"img\")\n",
        "        condition = condition_img.get_attribute(\"title\")\n",
        "\n",
        "        temperature = row.find_element(By.CSS_SELECTOR, \"td:nth-child(4)\").text\n",
        "\n",
        "        weather_list.append([city_name,condition,temperature])\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping a card due to error: {e}\")\n",
        "\n",
        "browser.quit()\n",
        "\n",
        "# Save as DataFrame\n",
        "weather = pd.DataFrame(weather_list, columns=[\"City_Name\", \"Condition\", \"Temperature\"])\n",
        "weather.to_csv(\"weather.csv\", index=False)\n",
        "print(weather.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFzyjlHelkC-",
        "outputId": "a91dadff-f6cd-4879-bd2f-fe4147292bef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (4.35.0)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "                           City_Name               Condition Temperature\n",
            "0                  Albania, Tirana *   Passing clouds. Warm.       82 °F\n",
            "1                   Algeria, Algiers  Scattered clouds. Hot.       91 °F\n",
            "2                     Angola, Luanda     Partly sunny. Mild.       73 °F\n",
            "3  Antigua and Barbuda, Saint John's   Passing clouds. Warm.       86 °F\n",
            "4            Argentina, Buenos Aires     Partly sunny. Cool.       59 °F\n"
          ]
        }
      ]
    }
  ]
}