{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1)\tThe Iris dataset is a classic example for demonstrating classification algorithms. It consists of 150 samples of iris flowers belonging to three species: Setosa, Versicolor, and Virginica, with four input features (sepal and petal length/width). Use SVC from sklearn.svm on the Iris dataset and follow the steps below:\n",
        "\n",
        "  a)\tLoad the dataset and perform train-test split (80:20).\n",
        "\n",
        "  b)\tTrain three different SVM models using the following kernels:\n",
        "  Linear, Polynomial (degree=3), RBF\n",
        "\n",
        "  c)\tEvaluate each model using:\n",
        "\n",
        "  •\tAccuracy\n",
        "\n",
        "  •\tPrecision\n",
        "\n",
        "  •\tRecall\n",
        "\n",
        "  •\tF1-Score\n",
        "\n",
        "  d)\tDisplay the confusion matrix for each kernel.\n",
        "\n",
        "  e)\tIdentify which kernel performs the best and why.\n"
      ],
      "metadata": {
        "id": "Fam7rSL3sRiR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSpUPniurM6J",
        "outputId": "4d05f56f-1a6f-44aa-c96c-538116db2d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Kernel: LINEAR =====\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  0 10]]\n",
            "\n",
            "===== Kernel: POLY =====\n",
            "Accuracy: 0.9\n",
            "Precision: 0.923076923076923\n",
            "Recall: 0.9\n",
            "F1-Score: 0.8976982097186701\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  3  7]]\n",
            "\n",
            "===== Kernel: RBF =====\n",
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9696969696969697\n",
            "Recall: 0.9666666666666667\n",
            "F1-Score: 0.9665831244778612\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  1]\n",
            " [ 0  0 10]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "kernels = [\"linear\", \"poly\", \"rbf\"]\n",
        "\n",
        "for k in kernels:\n",
        "    if k == \"poly\":\n",
        "        model = SVC(kernel=k, degree=3)\n",
        "    else:\n",
        "        model = SVC(kernel=k)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(f\"\\n===== Kernel: {k.upper()} =====\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred, average=\"macro\"))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred, average=\"macro\"))\n",
        "    print(\"F1-Score:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)\tSVM models are highly sensitive to the scale of input features. When features have different ranges, the algorithm may incorrectly assign higher importance to variables with larger magnitudes, affecting the placement of the separating hyperplane. Feature scaling ensures that all attributes contribute equally to distance-based computations, which is especially crucial for kernels like RBF or polynomial.\n",
        "\n",
        "  A) Use the Breast Cancer dataset from sklearn.datasets.load_breast_cancer.\n",
        "\n",
        "  B) Train an SVM (RBF kernel) model with and without feature scaling (StandardScaler). Compare both results using:\n",
        "  \n",
        "  •\tTraining accuracy\n",
        "  \n",
        "  •\tTesting accuracy\n",
        "\n",
        "  C) Discuss the effect of feature scaling on SVM performance.\n"
      ],
      "metadata": {
        "id": "TIyUCmnTs3tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# ------------------------\n",
        "# Model WITHOUT Scaling\n",
        "# ------------------------\n",
        "model_no_scale = SVC(kernel=\"rbf\")\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "\n",
        "train_acc_no = model_no_scale.score(X_train, y_train)\n",
        "test_acc_no = model_no_scale.score(X_test, y_test)\n",
        "\n",
        "print(\"\\n=== WITHOUT SCALING ===\")\n",
        "print(\"Training Accuracy:\", train_acc_no)\n",
        "print(\"Testing Accuracy:\", test_acc_no)\n",
        "\n",
        "# ------------------------\n",
        "# Model WITH Scaling\n",
        "# ------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = SVC(kernel=\"rbf\")\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "train_acc_scaled = model_scaled.score(X_train_scaled, y_train)\n",
        "test_acc_scaled = model_scaled.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"\\n=== WITH SCALING ===\")\n",
        "print(\"Training Accuracy:\", train_acc_scaled)\n",
        "print(\"Testing Accuracy:\", test_acc_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAgTad8utBpW",
        "outputId": "d90ee97d-6f4c-4acc-98ad-8d7b3203a95c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== WITHOUT SCALING ===\n",
            "Training Accuracy: 0.9186813186813186\n",
            "Testing Accuracy: 0.9298245614035088\n",
            "\n",
            "=== WITH SCALING ===\n",
            "Training Accuracy: 0.9824175824175824\n",
            "Testing Accuracy: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVMs compute distances in feature space, and when one feature has a much larger numeric range than others, it dominates the distance calculation. That distorts the shape of the decision boundary and leads to poor performance, especially for nonlinear kernels like RBF.\n",
        "After scaling, all features contribute equally, leading to a smoother and more accurate separating hyperplane. This usually improves both training and testing accuracy, as seen in the results."
      ],
      "metadata": {
        "id": "ZG-gx3jTtMvR"
      }
    }
  ]
}